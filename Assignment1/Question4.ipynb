{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step1: Must execute these below cells. These contain functions which are used in que2. ( I have written those functions here also so that there is not dependency between python notebooks)\n",
    "(functions for unicode correction of token, finding characters, syllables and their counts from corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "swar = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ',  'ए', 'ऐ', 'ओ', 'औ','ॲ', 'ऑ','अं', 'अः']\n",
    "\n",
    "halant_vyanjana_map = {'क्': 'क', 'ख्': 'ख', 'ग्': 'ग', 'घ्': 'घ',\n",
    "                        'ङ्': 'ङ', 'च्': 'च', 'छ्': 'छ', 'ज्': 'ज',\n",
    "                        'झ्': 'झ', 'ञ्': 'ञ', 'ट्': 'ट', 'ठ्': 'ठ',\n",
    "                        'ड्': 'ड', 'ढ्': 'ढ', 'ण्': 'ण', 'त्': 'त',\n",
    "                        'थ्': 'थ', 'द्': 'द', 'ध्': 'ध', 'न्': 'न',\n",
    "                        'प्': 'प', 'फ्': 'फ', 'ब्': 'ब', 'भ्': 'भ',\n",
    "                        'म्': 'म', 'य्': 'य', 'र्': 'र', 'ल्': 'ल',\n",
    "                        'व्': 'व', 'श्': 'श', 'ष्': 'ष', 'स्': 'स',\n",
    "                        'ह्': 'ह', 'ळ्': 'ळ', 'क्ष्': 'क्ष', 'ज्ञ्': 'ज्ञ'}\n",
    "\n",
    "\n",
    "swar_matra_map={'ा':'आ', 'ि':'इ', 'ी':'ई', 'ु':'उ', 'ू':'ऊ', 'े':'ए', 'ै':'ऐ', 'ो':'ओ', 'ौ': 'औ', 'ॅ':'ॲ', 'ॉ':'ऑ',  'ं':'अं', 'ः': 'अः'}\n",
    "reversed_swar_matra_map = {'आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ', 'ॲ': 'ॅ', 'ऑ': 'ॉ', 'अं': 'ं', 'अः': 'ः'}\n",
    "\n",
    "\n",
    "specials= { 'ँ':['ॲ','अं']}\n",
    "\n",
    "\n",
    "vyanjana_map={'क': 'क्', 'ख': 'ख्', 'ग': 'ग्', 'घ': 'घ्', \n",
    "                        'ङ': 'ङ्', 'च': 'च्', 'छ': 'छ्', 'ज': 'ज्', \n",
    "                        'झ': 'झ्', 'ञ': 'ञ्', 'ट': 'ट्', 'ठ': 'ठ्', \n",
    "                        'ड': 'ड्', 'ढ': 'ढ्', 'ण': 'ण्', 'त': 'त्', \n",
    "                        'थ': 'थ्', 'द': 'द्', 'ध': 'ध्', 'न': 'न्', \n",
    "                        'प': 'प्', 'फ': 'फ्', 'ब': 'ब्', 'भ': 'भ्', \n",
    "                        'म': 'म्', 'य': 'य्', 'र': 'र्', 'ल': 'ल्', \n",
    "                        'व': 'व्', 'श': 'श्', 'ष': 'ष्', 'स': 'स्', \n",
    "                        'ह': 'ह्', 'ळ': 'ळ्', 'क्ष': 'क्ष्', 'ज्ञ': 'ज्ञ्'}\n",
    "\n",
    "#function of unicode correction for question 2. This is to handle \" \"spaces also\n",
    "def unicode_correction(user_input):     #input can be word or sentence\n",
    "    corrected_unicodes=[]\n",
    "    length=len(user_input)\n",
    "    for i in range (length):\n",
    "        if(user_input[i] in swar):\n",
    "            corrected_unicodes.append(user_input[i])\n",
    "        elif(user_input[i] in vyanjana_map):   #if it is already a halant character e.g. त् is represented as 'त', '्' in given input\n",
    "            if(i<length-1 and (user_input[i+1]=='्' or user_input[i+1] in swar_matra_map)):  #i.e. if vyanjan is either halant or it has matra then \n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]])                          #writing only halant vyanjan \n",
    "\n",
    "            elif(i<length-1 and user_input[i+1] in specials):       #to handle बँ i.e. 'ब', 'ँ' this kind of thing. In this case I just have to insert ब् and not 'ब्' 'अ'\n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]])\n",
    "            else:                                                   #if vyanjan is actually a full vyanjan then\n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]]) #writing halant vyanjan and\n",
    "                corrected_unicodes.append('अ')                      #also writing swar अ after that\n",
    "\n",
    "        elif(user_input[i] in swar_matra_map):  #if character is some kind of matra then replacing it with swar\n",
    "            corrected_unicodes.append(swar_matra_map[user_input[i]])\n",
    "\n",
    "        elif (user_input[i] in specials):   #to handle 'ँ', we need to explicitly insert all swar associated with it. which is stored in specials\n",
    "            for j in specials[user_input[i]]:\n",
    "                corrected_unicodes.append(j)\n",
    "        elif(user_input[i]==\" \"):\n",
    "            corrected_unicodes.append(\" \")\n",
    "    return corrected_unicodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate count of characters\n",
    "def character_count(user_input2):\n",
    "    char_count={}                                           #to store character and their count\n",
    "    #print(\"Actual input is: \", list(user_input2))\n",
    "    correct_input2=unicode_correction(user_input2)         #first performing unicode correction to get actual characters count\n",
    "    #print(\"Corrected input is: \", correct_input2)\n",
    "    input2_len=len(correct_input2)\n",
    "    for i in range(input2_len):                             #iterating over corrected unicode\n",
    "        if(correct_input2[i]!= \" \"):                          #as we are not counting whitespace as character\n",
    "            if(correct_input2[i] not in char_count):            #if current character isn't present in char_count dictionary then inserting it   \n",
    "                char_count[correct_input2[i]]=1\n",
    "            else:                                               #if present then just increasing the count\n",
    "                char_count[correct_input2[i]]+=1                \n",
    "    return char_count\n",
    "\n",
    "def bigram_characters(user_input):               #same as character Count function. Just some Modifications for bigram characters\n",
    "    bigram_char_count={}\n",
    "    #print(\"Actual input: \", user_input)\n",
    "    correct_input=unicode_correction(user_input)\n",
    "    #print(\"Corrected input is: \", correct_input)\n",
    "    input_len=len(correct_input)\n",
    "    sentence_Bigram_chars=[]       #Just in case if we want to view all Bigram characters in sequencial order of occurance in sentence\n",
    "    \n",
    "    temp=\"\"             #to store last characters of previous BigramCharacters so that it can be used as first character for next BigramCharacters\n",
    "    bigram_char=\"\"   \n",
    "\n",
    "    #just look, itis almost similar to character_count function. Just minor changes.\n",
    "\n",
    "    i=0\n",
    "    while(i< input_len):\n",
    "        if(correct_input[i]==\" \"):\n",
    "            temp=\"\"     #emptying temp because word is completed. Hence Last syllable of word is not useful for next bigramSyllable\n",
    "            bigram_char=\"\"\n",
    "            i+=1\n",
    "        elif(correct_input[i] in swar or correct_input[i] in halant_vyanjana_map):                            \n",
    "            current_character=correct_input[i]\n",
    "            i+=1\n",
    "            temp=current_character           #storing current character as it may be used as first character in next bigramcharacter\n",
    "            if(len(bigram_char)>0):          #if it is not first bigram_character of the word. i.e. temp is already present as first character of current bigramcharacter\n",
    "                bigram_char+=current_character        #then just insert next character in bigramcharacter. Now this bigram is completed\n",
    "                \n",
    "                if(bigram_char not in bigram_char_count):        #just to store count of bigramcharacter\n",
    "                    bigram_char_count[bigram_char]=1\n",
    "                else:\n",
    "                    bigram_char_count[bigram_char]+=1\n",
    "\n",
    "                sentence_Bigram_chars.append(bigram_char)          #complete Bigram character inserting in list       \n",
    "\n",
    "                bigram_char=temp                 #Bigram is completed. The Current character i.e. last character of our completed bigram is now first character of next bigram\n",
    "            else:\n",
    "                bigram_char=current_character     #It is first character of the word. Hence store it\n",
    "                temp=\"\"                         #as it won't be used for next bigram, just erase temp\n",
    "                                            \n",
    "    return (bigram_char_count,sentence_Bigram_chars) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 2: Working with syllables. \n",
    "\n",
    "#Function to store count of Syllables or unigram Syllables\n",
    "def syllable_count(user_input2):\n",
    "    syllable_count={}\n",
    "    correct_input2=unicode_correction(user_input2)\n",
    "    input2_len=len(correct_input2)\n",
    "    current_syllable=\"\"\n",
    "    i=0\n",
    "    while(i< input2_len):\n",
    "        if(correct_input2[i]==\" \"):\n",
    "            current_syllable=\"\"\n",
    "            i+=1\n",
    "        elif(correct_input2[i] in swar):                          #first we are checking if there is only swars continuously occuring or not e.g. in ओंकार : ओं as ओ अं \n",
    "            while(i<input2_len and correct_input2[i] in swar):  #iterating over all swars continuously to make one swar syllable e.g. ओं\n",
    "                current_syllable+=correct_input2[i]\n",
    "                i+=1\n",
    "            if(current_syllable not in syllable_count):\n",
    "                syllable_count[current_syllable]=1\n",
    "            else:\n",
    "                syllable_count[current_syllable]+=1\n",
    "            current_syllable=\"\"                                 #As syllable count is inserted into dictionary hence\n",
    "\n",
    "        elif(correct_input2[i] in halant_vyanjana_map):         #if syllable starting with vyanjana then we consider \n",
    "                                                                #all halant vyanjana +all swars following that. \n",
    "                                                                #after that when new vyanjan comes I will stop. Same thing is written in comments below step by step\n",
    "\n",
    "            while(i<input2_len and correct_input2[i] in halant_vyanjana_map):   #syllable starting with vyanjana. hence consider all vyanjana\n",
    "                current_syllable+=correct_input2[i]                             \n",
    "                i+=1   \n",
    "            current_syllable=current_syllable[:-1]              #last vyanjana of that should be full vyanjana so that we can use all matras on that\n",
    "                                                                #Hence I am removing halant from last vyanjana to make it full vyanjana\n",
    "                                                                #e.g. क् is stored as 'क' + '्' Hence removing '्' from last halant vyanjana to make it\n",
    "                                                                #full vyanjana\n",
    "            \n",
    "            while(i<input2_len and correct_input2[i] in swar):\n",
    "                if(correct_input2[i]=='अ'):                     #ignoring 'अ' because i have already made last vyanjana as full vyanjana\n",
    "                    i=i+1\n",
    "                    continue\n",
    "                current_syllable+=reversed_swar_matra_map[correct_input2[i]]  #inserting matra of that swar e.g. if swar is ई then inserting 'ी'\n",
    "                i+=1\n",
    "            if(current_syllable not in syllable_count):         #storing count of syllable in dictionary\n",
    "                syllable_count[current_syllable]=1\n",
    "            else:\n",
    "                syllable_count[current_syllable]+=1\n",
    "            current_syllable=\"\"                                 #current syllable completed hence start new syllable with empty string\n",
    "    return syllable_count\n",
    "\n",
    "def bigram_syllable(user_input2):               #same as Syllable Count function. Just some Modifications for bigram Syllables\n",
    "    bigram_syllable_count={}\n",
    "    #print(\"Actual input: \", user_input2)\n",
    "    correct_input2=unicode_correction(user_input2)\n",
    "    #print(\"Corrected input is: \", correct_input2)\n",
    "    input2_len=len(correct_input2)\n",
    "    sentence_Bigram_syllables=[]       #Just in case if we want to view all Bigram syllables in sequencial order of occurance in sentence\n",
    "    \n",
    "    temp=\"\"             #to store last syllable of previous bigramSyllable so that it can be used as first syllable for next bigramSyllable\n",
    "    bigram_syl=\"\"   \n",
    "\n",
    "    #just look, itis almost similar to syllable_count function. Just minor changes.\n",
    "    #if not able to understand. See bigram Character Count function code. Logic is same here also\n",
    "    current_syllable=\"\"\n",
    "    i=0\n",
    "    while(i< input2_len):\n",
    "        if(correct_input2[i]==\" \"):\n",
    "            current_syllable=\"\"\n",
    "            temp=\"\"     #emptying temp because word is completed. Hence Last syllable of word is not useful for next bigramSyllable\n",
    "            bigram_syl=\"\"\n",
    "            i+=1\n",
    "        elif(correct_input2[i] in swar):                          \n",
    "            while(i<input2_len and correct_input2[i] in swar):  \n",
    "                current_syllable+=correct_input2[i]\n",
    "                i+=1\n",
    "\n",
    "            #This code is specific to bigram Syllables function   \n",
    "            temp=current_syllable           #storing current syllable as it may be used as first syllable in next bigramSyllable\n",
    "            if(len(bigram_syl)>0):          #if it is not first bigram_syllable of the word. i.e. temp is already present as first syllable of current bigramSyllable\n",
    "                bigram_syl+=current_syllable        #then just insert next syllable in bigramSyllable. Now this bigram is completed\n",
    "                \n",
    "                if(bigram_syl not in bigram_syllable_count):        #just to store count of bigramSyllables\n",
    "                    bigram_syllable_count[bigram_syl]=1\n",
    "                else:\n",
    "                    bigram_syllable_count[bigram_syl]+=1\n",
    "\n",
    "                sentence_Bigram_syllables.append(bigram_syl)          #complete Bigram syllable inserting in list       \n",
    "\n",
    "                bigram_syl=temp                 #Bigram is completed. The Current Syllable i.e. last syllable of our completed bigram is now first syllable of next bigram\n",
    "            else:\n",
    "                bigram_syl=current_syllable     #It is first syllable of the word. Hence store it\n",
    "                temp=\"\"                         #as it won't be used for next bigram, just erase temp\n",
    "            \n",
    "            #till here\n",
    "                \n",
    "        #Following code is same as syllable_count function\n",
    "            current_syllable=\"\"                                  \n",
    "\n",
    "        elif(correct_input2[i] in halant_vyanjana_map):         \n",
    "            while(i<input2_len and correct_input2[i] in halant_vyanjana_map):   \n",
    "                current_syllable+=correct_input2[i]                             \n",
    "                i+=1   \n",
    "            current_syllable=current_syllable[:-1]              \n",
    "                                                                \n",
    "            while(i<input2_len and correct_input2[i] in swar):\n",
    "                if(correct_input2[i]=='अ'):                     \n",
    "                    i=i+1\n",
    "                    continue\n",
    "                current_syllable+=reversed_swar_matra_map[correct_input2[i]]  \n",
    "                i+=1\n",
    "        #Till Here\n",
    "                \n",
    "\n",
    "            #Following code is specific to bigram Syllables function               \n",
    "            temp=current_syllable              #storing current syllable as it may be used as first syllable in next bigramSyllable\n",
    "            if(len(bigram_syl)>0):              #if it is not first bigram_syllable of the word. i.e. temp is already present as first syllable of current bigramSyllable\n",
    "                bigram_syl+=current_syllable    #then just insert next syllable in bigramSyllable. Now this bigram is completed\n",
    "                # print(\" this bigram: \", bigram_syl)\n",
    "\n",
    "                if(bigram_syl not in bigram_syllable_count):    #just to store count of bigramSyllables\n",
    "                    bigram_syllable_count[bigram_syl]=1\n",
    "                else:\n",
    "                    bigram_syllable_count[bigram_syl]+=1\n",
    "\n",
    "                sentence_Bigram_syllables.append(bigram_syl)          #complete Bigram syllable inserting in list\n",
    "\n",
    "                bigram_syl=temp                #Bigram is completed. The Current Syllable i.e. last syllable of our completed bigram is now first syllable of next bigram\n",
    "            else:\n",
    "                bigram_syl=current_syllable     #It is first syllable of the word. Hence store it\n",
    "                temp=\"\"                            #as it won't be used for next bigram, just erase temp\n",
    "            \n",
    "            #till here\n",
    "            current_syllable=\"\"                                 #as complete syllable if found\n",
    "    \n",
    "\n",
    "    return (bigram_syllable_count,sentence_Bigram_syllables) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step2: Reading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path='corpus/mr_100.txt'\n",
    "with open(corpus_path, 'r', encoding='utf-8',errors='ignore') as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select only one toknizer at a time. \n",
    "if you want to try different tokanizers then execute any one tokenizer of step 3 and execute all cells of step4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using BPE tokanizer (then only execute below cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run below cell for BPE Tokenizer with V=1000:  \n",
    "(if you want to use another corpus then provide the path in below cell also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training BPE model with vocabulary size V = 1000\n",
    "\n",
    "#This took around 4 minues to execute this cell\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/mr_100.txt --model_prefix=bpe_model_1000 --model_type=bpe --vocab_size=1000')\n",
    "#Loading BPE model (V = 1000)\n",
    "sp_bpe_1000 = spm.SentencePieceProcessor()\n",
    "sp_bpe_1000.load('bpe_model_1000.model')\n",
    "#Tokenizing with BPE model (V = 1000)\n",
    "extracted_tokens = sp_bpe_1000.encode_as_pieces(corpus)\n",
    "print(\"Displaying some Tokens (BPE V=1000): \",extracted_tokens[:10])\n",
    "\n",
    "storage_path=\"BPE_V1000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run below cell for BPE Tokenizer with V=2000 \n",
    "if you want to use another corpus then provide the path in below cell also\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens (BPE V=2000):  ['▁स्व', 'प्', 'न', '▁दाख', 'वि', 'णे', '▁आणि', '▁आ', 'श्', 'वास']\n"
     ]
    }
   ],
   "source": [
    "#Training BPE model with vocabulary size V = 2000\n",
    "\n",
    "#This took around 6 minues to execute this cell\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/mr_100.txt --model_prefix=bpe_model_2000 --model_type=bpe --vocab_size=2000')\n",
    "\n",
    "#Loading BPE model (V = 2000)\n",
    "sp_bpe_2000 = spm.SentencePieceProcessor()\n",
    "sp_bpe_2000.load('bpe_model_2000.model')\n",
    "\n",
    "#Tokenizing with BPE model (V = 2000)\n",
    "extracted_tokens = sp_bpe_2000.encode_as_pieces(corpus)\n",
    "print(\"Displaying some Tokens (BPE V=2000): \",extracted_tokens[:10])\n",
    "storage_path=\"BPE_V2000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using whitespace tokanizer (then only execute below cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (Whitespace Tokenizer): ['स्वप्न', 'दाखविणे', 'आणि', 'आश्वासने', 'देणारे', 'नेतेही', 'लोकांना', 'खूप', 'आवडतात.', 'मुलांसोबतच']\n"
     ]
    }
   ],
   "source": [
    "extracted_tokens = corpus.split()\n",
    "print(\"Tokens (Whitespace Tokenizer):\", extracted_tokens[:10])\n",
    "storage_path=\"Whitespace_Tokenizer/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using Unigram tokanizer (then only execute below cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram tokenizer with V=1000 \n",
    "(if you want to use another corpus then provide the path in below cell also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the extracted Tokens: ['▁स्व', 'प', '्', 'न', '▁दाखव', 'ि', 'णे', '▁आणि', '▁आ', 'श्']\n"
     ]
    }
   ],
   "source": [
    "#training Unigram model with vocab size V = 1000\n",
    "\n",
    "#if want to use another corpus. then provide path in input=\"\" in below line\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/mr_100.txt --model_prefix=unigram_model --model_type=unigram --vocab_size=1000')\n",
    "\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load('unigram_model.model')\n",
    "\n",
    "#Tokenization with Unigram model\n",
    "extracted_tokens = sp_unigram.encode_as_pieces(corpus)\n",
    "\n",
    "print(\"Some of the extracted Tokens:\", extracted_tokens[:10])\n",
    "storage_path=\"Unigram_Tokenizer_V1000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram tokenizer with V=2000 \n",
    "(if you want to use another corpus then provide the path in below cell also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the extracted Tokens: ['▁स्वप्न', '▁दाखव', 'ि', 'णे', '▁आणि', '▁आश्वासन', 'े', '▁देणार', 'े', '▁नेते']\n"
     ]
    }
   ],
   "source": [
    "#training Unigram model with vocab size V = 2000\n",
    "\n",
    "#if want to use another corpus. then provide path in input=\"\" in below line\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/mr_100.txt --model_prefix=unigram_model --model_type=unigram --vocab_size=2000')\n",
    "\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load('unigram_model.model')\n",
    "\n",
    "#Tokenization with Unigram model\n",
    "extracted_tokens = sp_unigram.encode_as_pieces(corpus)\n",
    "\n",
    "print(\"Some of the extracted Tokens:\", extracted_tokens[:10])\n",
    "storage_path=\"Unigram_Tokenizer_V2000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization using mBERT tokanizer (then only execute below cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run below cell for mBERT with max_length=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\anaconda3\\envs\\iitkMtech\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the Tokens: ['[CLS]', 'स', '##्व', '##प', '##्न', 'द', '##ा', '##ख', '##वि', '##णे']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#Loading tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#Tokenizing the corpus with max length = 1000\n",
    "tokens_ids = tokenizer.encode(corpus, max_length=1000, truncation=True)\n",
    "\n",
    "#getting tokens from ids\n",
    "extracted_tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "print(\"Some of the Tokens:\", extracted_tokens[:10])\n",
    "storage_path=\"mBERT_word1000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run below line for mBERT with max_length=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the Tokens: ['[CLS]', 'स', '##्व', '##प', '##्न', 'द', '##ा', '##ख', '##वि', '##णे']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#Loading tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#Tokenizing the corpus with max length = 2000\n",
    "tokens_ids = tokenizer.encode(corpus, max_length=2000, truncation=True)\n",
    "\n",
    "#getting tokens from ids\n",
    "extracted_tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "print(\"Some of the Tokens:\", extracted_tokens[:10])\n",
    "storage_path=\"mBERT_word2000/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Execute all cells below to get required frequencies. \n",
    "tokenization is completed by model of our choice. \n",
    "Now, I am storing calculated frequencies in /frequencies folder. However i am not creating seperate files for frequencies calculated by each tokenizer. Instead these files will be overwritten. \n",
    "So, if you want to see frequency using another tokenizer: Then do following\n",
    "\n",
    "1) Just execute cells of tokenizer of your choice mentioned above  \n",
    "2) Run all cells below\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens after unicode correction:  ['स्व्अप्न्अ', 'द्आख्अव्इण्ए', 'आण्इ', 'आश्व्आस्अन्ए', 'द्एण्आर्ए', 'न्एत्एह्ई', 'ल्ओक्आअंन्आ', 'ख्ऊप्अ', 'आव्अड्अत्आत्अ', 'म्उल्आअंस्ओब्अत्अच्अ']\n"
     ]
    }
   ],
   "source": [
    "#performing unicode correction on each token and storing it back\n",
    "\n",
    "for i in range(len(extracted_tokens)):\n",
    "    extracted_tokens[i]=unicode_correction(extracted_tokens[i])\n",
    "    extracted_tokens[i] = ''.join(extracted_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "extracted_tokens = [token for token in extracted_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",extracted_tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unigram frequencies of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counter = Counter(extracted_tokens)\n",
    "output_directory = \"frequencies/\"+storage_path\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "BPE_V1000_path = os.path.join(output_directory, 'unigram_tokens.json')\n",
    "Desc_unigram_token_count = dict(sorted(unigram_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "readable_unigram_tokens = json.dumps(Desc_unigram_token_count, ensure_ascii=False, indent=2)\n",
    "with open(BPE_V1000_path, 'w') as json_file:\n",
    "    json_file.write(readable_unigram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### biigram frequencies of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_tokens = zip(extracted_tokens, extracted_tokens[1:])\n",
    "# counting bigram frequencies\n",
    "bigram_counter = Counter(bigram_tokens)\n",
    "#print(bigram_counter)\n",
    "# Converting tuple keys to strings for JSON serialization\n",
    "bigram_counter = {str(key): value for key, value in bigram_counter.items()}\n",
    "\n",
    "\n",
    "#Storing bigram frequencies of tokens in a JSON file in local machine\n",
    "output_directory = \"frequencies/\"+storage_path\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "Desc_bigram_token_count = dict(sorted(bigram_counter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "readable_bigram_tokens = json.dumps(Desc_bigram_token_count, ensure_ascii=False, indent=2)\n",
    "BPE_V1000_path = os.path.join(output_directory, 'bigram_tokens.json')\n",
    "with open(BPE_V1000_path, 'w') as json_file:\n",
    "    json_file.write(readable_bigram_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram frequencies of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#execution of this cell takes some time (it took around 4 minutes on my laptop)\n",
    "Bigram_Char_counts={}\n",
    "sequential_bigram_char_corpus=[]\n",
    "\n",
    "for token in extracted_tokens:\n",
    "    if not token:        #empty token\n",
    "        continue\n",
    "    #print(sentence)\n",
    "    token_char_count=character_count(token)\n",
    "    sentence_Bigramchar_count,bigram_chars_in_sentences=bigram_characters(token)\n",
    "\n",
    "    sequential_bigram_char_corpus.append(bigram_chars_in_sentences)        #appending bigram characters of token to toal bigram characters of corpus list\n",
    "    for key in sentence_Bigramchar_count:\n",
    "        if(key in Bigram_Char_counts):\n",
    "            Bigram_Char_counts[key]+=sentence_Bigramchar_count[key]\n",
    "        else:\n",
    "            Bigram_Char_counts[key]=sentence_Bigramchar_count[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting in descending order of their frequencies \n",
    "\n",
    "#print(\"bigramCharacters of first 10 tokens of corpus: \",sequential_bigram_char_corpus[:10])\n",
    "Desc_bigramChar_count = dict(sorted(Bigram_Char_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(\"Descending order of count of bigram characters (Showing top 20): \", dict(list(Desc_bigramChar_count.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing bigram frequencies of characters in a JSON file in local machine\n",
    "output_directory = \"frequencies/\"+storage_path\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "formatted_content_bigram_chars = json.dumps(Desc_bigramChar_count, ensure_ascii=False, indent=2)\n",
    "BPE_V1000_path_bigram_chars = os.path.join(output_directory, 'bigram_chars.json')\n",
    "with open(BPE_V1000_path_bigram_chars, 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(formatted_content_bigram_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bigram frequencies of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell takes around 3 minues to run in my laptop \n",
    "Bigram_syl_counts={}        #to measure overall bigram syllable count of corpus\n",
    "sequential_bigram_syl_corpus=[]     #to get all bigram syllables in same order of corpus\n",
    "\n",
    "for token in extracted_tokens:\n",
    "    if not token:        #empty line\n",
    "        continue\n",
    "    bigram_syl_in_token=[]\n",
    "    #print(sentence)\n",
    "    token_syl_count=syllable_count(token)\n",
    "    token_bigramSyl_count,bigram_syl_in_token=bigram_syllable(token)      \n",
    "\n",
    "    sequential_bigram_syl_corpus.append(bigram_syl_in_token)        #appending bigram syllable of tokens to bigram syllables of corpus list\n",
    "    \n",
    "    for key in token_bigramSyl_count:\n",
    "        if(key in Bigram_syl_counts):\n",
    "            Bigram_syl_counts[key]+=token_bigramSyl_count[key]\n",
    "        else:\n",
    "            Bigram_syl_counts[key]=token_bigramSyl_count[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"bigramSyllables of first 20 tokens of corpus: \",sequential_bigram_syl_corpus[:20])\n",
    "Desc_bigramSyl_count = dict(sorted(Bigram_syl_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "#print(\"Descending order of count of bigram Syllables (Showing top 20): \", dict(list(Desc_bigramSyl_count.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing bigram frequencies of syllables in a JSON file in local machine\n",
    "output_directory = \"frequencies/\"+storage_path\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "formatted_content_bigram_syllables = json.dumps(Desc_bigramSyl_count, ensure_ascii=False, indent=2)\n",
    "BPE_V1000_path_bigram_syllables = os.path.join(output_directory, 'bigram_syllables.json')\n",
    "with open(BPE_V1000_path_bigram_syllables, 'w', encoding='utf-8') as json_file:\n",
    "    json_file.write(formatted_content_bigram_syllables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitkMtech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
