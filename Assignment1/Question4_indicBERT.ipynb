{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN6t60bCzekl"
      },
      "source": [
        "### This is a collab notebook.  \n",
        "indicBERT gives error while loading indicBERT model on localhost.  \n",
        "Hence I implemented this part of question4 on google collab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpRAcQvs1c0n",
        "outputId": "963ed984-6d44-491a-c636-b5ecccfd29b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hkiW6M5TZkbS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdzASd3t2zgK"
      },
      "source": [
        "##### Step1: Run These cells. These cells contains function of unicode correction, unigram and bigram character and syllable count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iaGYLdkQ2VM_"
      },
      "outputs": [],
      "source": [
        "swar = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ',  'ए', 'ऐ', 'ओ', 'औ','ॲ', 'ऑ','अं', 'अः']\n",
        "\n",
        "halant_vyanjana_map = {'क्': 'क', 'ख्': 'ख', 'ग्': 'ग', 'घ्': 'घ',\n",
        "                        'ङ्': 'ङ', 'च्': 'च', 'छ्': 'छ', 'ज्': 'ज',\n",
        "                        'झ्': 'झ', 'ञ्': 'ञ', 'ट्': 'ट', 'ठ्': 'ठ',\n",
        "                        'ड्': 'ड', 'ढ्': 'ढ', 'ण्': 'ण', 'त्': 'त',\n",
        "                        'थ्': 'थ', 'द्': 'द', 'ध्': 'ध', 'न्': 'न',\n",
        "                        'प्': 'प', 'फ्': 'फ', 'ब्': 'ब', 'भ्': 'भ',\n",
        "                        'म्': 'म', 'य्': 'य', 'र्': 'र', 'ल्': 'ल',\n",
        "                        'व्': 'व', 'श्': 'श', 'ष्': 'ष', 'स्': 'स',\n",
        "                        'ह्': 'ह', 'ळ्': 'ळ', 'क्ष्': 'क्ष', 'ज्ञ्': 'ज्ञ'}\n",
        "\n",
        "\n",
        "swar_matra_map={'ा':'आ', 'ि':'इ', 'ी':'ई', 'ु':'उ', 'ू':'ऊ', 'े':'ए', 'ै':'ऐ', 'ो':'ओ', 'ौ': 'औ', 'ॅ':'ॲ', 'ॉ':'ऑ',  'ं':'अं', 'ः': 'अः'}\n",
        "reversed_swar_matra_map = {'आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ', 'ॲ': 'ॅ', 'ऑ': 'ॉ', 'अं': 'ं', 'अः': 'ः'}\n",
        "\n",
        "\n",
        "specials= { 'ँ':['ॲ','अं']}\n",
        "\n",
        "\n",
        "vyanjana_map={'क': 'क्', 'ख': 'ख्', 'ग': 'ग्', 'घ': 'घ्',\n",
        "                        'ङ': 'ङ्', 'च': 'च्', 'छ': 'छ्', 'ज': 'ज्',\n",
        "                        'झ': 'झ्', 'ञ': 'ञ्', 'ट': 'ट्', 'ठ': 'ठ्',\n",
        "                        'ड': 'ड्', 'ढ': 'ढ्', 'ण': 'ण्', 'त': 'त्',\n",
        "                        'थ': 'थ्', 'द': 'द्', 'ध': 'ध्', 'न': 'न्',\n",
        "                        'प': 'प्', 'फ': 'फ्', 'ब': 'ब्', 'भ': 'भ्',\n",
        "                        'म': 'म्', 'य': 'य्', 'र': 'र्', 'ल': 'ल्',\n",
        "                        'व': 'व्', 'श': 'श्', 'ष': 'ष्', 'स': 'स्',\n",
        "                        'ह': 'ह्', 'ळ': 'ळ्', 'क्ष': 'क्ष्', 'ज्ञ': 'ज्ञ्'}\n",
        "\n",
        "#function of unicode correction for question 2. This is to handle \" \"spaces also\n",
        "def unicode_correction(user_input):     #input can be word or sentence\n",
        "    corrected_unicodes=[]\n",
        "    length=len(user_input)\n",
        "    for i in range (length):\n",
        "        if(user_input[i] in swar):\n",
        "            corrected_unicodes.append(user_input[i])\n",
        "        elif(user_input[i] in vyanjana_map):   #if it is already a halant character e.g. त् is represented as 'त', '्' in given input\n",
        "            if(i<length-1 and (user_input[i+1]=='्' or user_input[i+1] in swar_matra_map)):  #i.e. if vyanjan is either halant or it has matra then\n",
        "                corrected_unicodes.append(vyanjana_map[user_input[i]])                          #writing only halant vyanjan\n",
        "\n",
        "            elif(i<length-1 and user_input[i+1] in specials):       #to handle बँ i.e. 'ब', 'ँ' this kind of thing. In this case I just have to insert ब् and not 'ब्' 'अ'\n",
        "                corrected_unicodes.append(vyanjana_map[user_input[i]])\n",
        "            else:                                                   #if vyanjan is actually a full vyanjan then\n",
        "                corrected_unicodes.append(vyanjana_map[user_input[i]]) #writing halant vyanjan and\n",
        "                corrected_unicodes.append('अ')                      #also writing swar अ after that\n",
        "\n",
        "        elif(user_input[i] in swar_matra_map):  #if character is some kind of matra then replacing it with swar\n",
        "            corrected_unicodes.append(swar_matra_map[user_input[i]])\n",
        "\n",
        "        elif (user_input[i] in specials):   #to handle 'ँ', we need to explicitly insert all swar associated with it. which is stored in specials\n",
        "            for j in specials[user_input[i]]:\n",
        "                corrected_unicodes.append(j)\n",
        "        elif(user_input[i]==\" \"):\n",
        "            corrected_unicodes.append(\" \")\n",
        "    return corrected_unicodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ehIMxnH_2pil"
      },
      "outputs": [],
      "source": [
        "#Function to calculate count of characters\n",
        "def character_count(user_input2):\n",
        "    char_count={}                                           #to store character and their count\n",
        "    #print(\"Actual input is: \", list(user_input2))\n",
        "    correct_input2=unicode_correction(user_input2)         #first performing unicode correction to get actual characters count\n",
        "    #print(\"Corrected input is: \", correct_input2)\n",
        "    input2_len=len(correct_input2)\n",
        "    for i in range(input2_len):                             #iterating over corrected unicode\n",
        "        if(correct_input2[i]!= \" \"):                          #as we are not counting whitespace as character\n",
        "            if(correct_input2[i] not in char_count):            #if current character isn't present in char_count dictionary then inserting it\n",
        "                char_count[correct_input2[i]]=1\n",
        "            else:                                               #if present then just increasing the count\n",
        "                char_count[correct_input2[i]]+=1\n",
        "    return char_count\n",
        "\n",
        "def bigram_characters(user_input):               #same as character Count function. Just some Modifications for bigram characters\n",
        "    bigram_char_count={}\n",
        "    #print(\"Actual input: \", user_input)\n",
        "    correct_input=unicode_correction(user_input)\n",
        "    #print(\"Corrected input is: \", correct_input)\n",
        "    input_len=len(correct_input)\n",
        "    sentence_Bigram_chars=[]       #Just in case if we want to view all Bigram characters in sequencial order of occurance in sentence\n",
        "\n",
        "    temp=\"\"             #to store last characters of previous BigramCharacters so that it can be used as first character for next BigramCharacters\n",
        "    bigram_char=\"\"\n",
        "\n",
        "    #just look, itis almost similar to character_count function. Just minor changes.\n",
        "\n",
        "    i=0\n",
        "    while(i< input_len):\n",
        "        if(correct_input[i]==\" \"):\n",
        "            temp=\"\"     #emptying temp because word is completed. Hence Last syllable of word is not useful for next bigramSyllable\n",
        "            bigram_char=\"\"\n",
        "            i+=1\n",
        "        elif(correct_input[i] in swar or correct_input[i] in halant_vyanjana_map):\n",
        "            current_character=correct_input[i]\n",
        "            i+=1\n",
        "            temp=current_character           #storing current character as it may be used as first character in next bigramcharacter\n",
        "            if(len(bigram_char)>0):          #if it is not first bigram_character of the word. i.e. temp is already present as first character of current bigramcharacter\n",
        "                bigram_char+=current_character        #then just insert next character in bigramcharacter. Now this bigram is completed\n",
        "\n",
        "                if(bigram_char not in bigram_char_count):        #just to store count of bigramcharacter\n",
        "                    bigram_char_count[bigram_char]=1\n",
        "                else:\n",
        "                    bigram_char_count[bigram_char]+=1\n",
        "\n",
        "                sentence_Bigram_chars.append(bigram_char)          #complete Bigram character inserting in list\n",
        "\n",
        "                bigram_char=temp                 #Bigram is completed. The Current character i.e. last character of our completed bigram is now first character of next bigram\n",
        "            else:\n",
        "                bigram_char=current_character     #It is first character of the word. Hence store it\n",
        "                temp=\"\"                         #as it won't be used for next bigram, just erase temp\n",
        "\n",
        "    return (bigram_char_count,sentence_Bigram_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v6jGJ4Eb247J"
      },
      "outputs": [],
      "source": [
        "#part 2: Working with syllables.\n",
        "\n",
        "#Function to store count of Syllables or unigram Syllables\n",
        "def syllable_count(user_input2):\n",
        "    syllable_count={}\n",
        "    correct_input2=unicode_correction(user_input2)\n",
        "    input2_len=len(correct_input2)\n",
        "    current_syllable=\"\"\n",
        "    i=0\n",
        "    while(i< input2_len):\n",
        "        if(correct_input2[i]==\" \"):\n",
        "            current_syllable=\"\"\n",
        "            i+=1\n",
        "        elif(correct_input2[i] in swar):                          #first we are checking if there is only swars continuously occuring or not e.g. in ओंकार : ओं as ओ अं\n",
        "            while(i<input2_len and correct_input2[i] in swar):  #iterating over all swars continuously to make one swar syllable e.g. ओं\n",
        "                current_syllable+=correct_input2[i]\n",
        "                i+=1\n",
        "            if(current_syllable not in syllable_count):\n",
        "                syllable_count[current_syllable]=1\n",
        "            else:\n",
        "                syllable_count[current_syllable]+=1\n",
        "            current_syllable=\"\"                                 #As syllable count is inserted into dictionary hence\n",
        "\n",
        "        elif(correct_input2[i] in halant_vyanjana_map):         #if syllable starting with vyanjana then we consider\n",
        "                                                                #all halant vyanjana +all swars following that.\n",
        "                                                                #after that when new vyanjan comes I will stop. Same thing is written in comments below step by step\n",
        "\n",
        "            while(i<input2_len and correct_input2[i] in halant_vyanjana_map):   #syllable starting with vyanjana. hence consider all vyanjana\n",
        "                current_syllable+=correct_input2[i]\n",
        "                i+=1\n",
        "            current_syllable=current_syllable[:-1]              #last vyanjana of that should be full vyanjana so that we can use all matras on that\n",
        "                                                                #Hence I am removing halant from last vyanjana to make it full vyanjana\n",
        "                                                                #e.g. क् is stored as 'क' + '्' Hence removing '्' from last halant vyanjana to make it\n",
        "                                                                #full vyanjana\n",
        "\n",
        "            while(i<input2_len and correct_input2[i] in swar):\n",
        "                if(correct_input2[i]=='अ'):                     #ignoring 'अ' because i have already made last vyanjana as full vyanjana\n",
        "                    i=i+1\n",
        "                    continue\n",
        "                current_syllable+=reversed_swar_matra_map[correct_input2[i]]  #inserting matra of that swar e.g. if swar is ई then inserting 'ी'\n",
        "                i+=1\n",
        "            if(current_syllable not in syllable_count):         #storing count of syllable in dictionary\n",
        "                syllable_count[current_syllable]=1\n",
        "            else:\n",
        "                syllable_count[current_syllable]+=1\n",
        "            current_syllable=\"\"                                 #current syllable completed hence start new syllable with empty string\n",
        "    return syllable_count\n",
        "\n",
        "def bigram_syllable(user_input2):               #same as Syllable Count function. Just some Modifications for bigram Syllables\n",
        "    bigram_syllable_count={}\n",
        "    #print(\"Actual input: \", user_input2)\n",
        "    correct_input2=unicode_correction(user_input2)\n",
        "    #print(\"Corrected input is: \", correct_input2)\n",
        "    input2_len=len(correct_input2)\n",
        "    sentence_Bigram_syllables=[]       #Just in case if we want to view all Bigram syllables in sequencial order of occurance in sentence\n",
        "\n",
        "    temp=\"\"             #to store last syllable of previous bigramSyllable so that it can be used as first syllable for next bigramSyllable\n",
        "    bigram_syl=\"\"\n",
        "\n",
        "    #just look, itis almost similar to syllable_count function. Just minor changes.\n",
        "    #if not able to understand. See bigram Character Count function code. Logic is same here also\n",
        "    current_syllable=\"\"\n",
        "    i=0\n",
        "    while(i< input2_len):\n",
        "        if(correct_input2[i]==\" \"):\n",
        "            current_syllable=\"\"\n",
        "            temp=\"\"     #emptying temp because word is completed. Hence Last syllable of word is not useful for next bigramSyllable\n",
        "            bigram_syl=\"\"\n",
        "            i+=1\n",
        "        elif(correct_input2[i] in swar):\n",
        "            while(i<input2_len and correct_input2[i] in swar):\n",
        "                current_syllable+=correct_input2[i]\n",
        "                i+=1\n",
        "\n",
        "            #This code is specific to bigram Syllables function\n",
        "            temp=current_syllable           #storing current syllable as it may be used as first syllable in next bigramSyllable\n",
        "            if(len(bigram_syl)>0):          #if it is not first bigram_syllable of the word. i.e. temp is already present as first syllable of current bigramSyllable\n",
        "                bigram_syl+=current_syllable        #then just insert next syllable in bigramSyllable. Now this bigram is completed\n",
        "\n",
        "                if(bigram_syl not in bigram_syllable_count):        #just to store count of bigramSyllables\n",
        "                    bigram_syllable_count[bigram_syl]=1\n",
        "                else:\n",
        "                    bigram_syllable_count[bigram_syl]+=1\n",
        "\n",
        "                sentence_Bigram_syllables.append(bigram_syl)          #complete Bigram syllable inserting in list\n",
        "\n",
        "                bigram_syl=temp                 #Bigram is completed. The Current Syllable i.e. last syllable of our completed bigram is now first syllable of next bigram\n",
        "            else:\n",
        "                bigram_syl=current_syllable     #It is first syllable of the word. Hence store it\n",
        "                temp=\"\"                         #as it won't be used for next bigram, just erase temp\n",
        "\n",
        "            #till here\n",
        "\n",
        "        #Following code is same as syllable_count function\n",
        "            current_syllable=\"\"\n",
        "\n",
        "        elif(correct_input2[i] in halant_vyanjana_map):\n",
        "            while(i<input2_len and correct_input2[i] in halant_vyanjana_map):\n",
        "                current_syllable+=correct_input2[i]\n",
        "                i+=1\n",
        "            current_syllable=current_syllable[:-1]\n",
        "\n",
        "            while(i<input2_len and correct_input2[i] in swar):\n",
        "                if(correct_input2[i]=='अ'):\n",
        "                    i=i+1\n",
        "                    continue\n",
        "                current_syllable+=reversed_swar_matra_map[correct_input2[i]]\n",
        "                i+=1\n",
        "        #Till Here\n",
        "\n",
        "\n",
        "            #Following code is specific to bigram Syllables function\n",
        "            temp=current_syllable              #storing current syllable as it may be used as first syllable in next bigramSyllable\n",
        "            if(len(bigram_syl)>0):              #if it is not first bigram_syllable of the word. i.e. temp is already present as first syllable of current bigramSyllable\n",
        "                bigram_syl+=current_syllable    #then just insert next syllable in bigramSyllable. Now this bigram is completed\n",
        "                # print(\" this bigram: \", bigram_syl)\n",
        "\n",
        "                if(bigram_syl not in bigram_syllable_count):    #just to store count of bigramSyllables\n",
        "                    bigram_syllable_count[bigram_syl]=1\n",
        "                else:\n",
        "                    bigram_syllable_count[bigram_syl]+=1\n",
        "\n",
        "                sentence_Bigram_syllables.append(bigram_syl)          #complete Bigram syllable inserting in list\n",
        "\n",
        "                bigram_syl=temp                #Bigram is completed. The Current Syllable i.e. last syllable of our completed bigram is now first syllable of next bigram\n",
        "            else:\n",
        "                bigram_syl=current_syllable     #It is first syllable of the word. Hence store it\n",
        "                temp=\"\"                            #as it won't be used for next bigram, just erase temp\n",
        "\n",
        "            #till here\n",
        "            current_syllable=\"\"                                 #as complete syllable if found\n",
        "\n",
        "\n",
        "    return (bigram_syllable_count,sentence_Bigram_syllables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owD4P6p7yCrM"
      },
      "source": [
        "##### Step2: Reading corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xSkGb1Sg29dd"
      },
      "outputs": [],
      "source": [
        "#providing path of corpus\n",
        "#please provide path accordingly\n",
        "corpus_path=\"/content/gdrive/MyDrive/Mtech2ndSEM/CS689ComputationalLinguistic/TemporaryAssignment1/Assignment1collabnotebooks/corpus/mr_100.txt\"\n",
        "with open(corpus_path, 'r', encoding='utf-8',errors='ignore') as file:\n",
        "    corpus = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VJzu-3cxa6X"
      },
      "source": [
        "#### Step3: Select only one tokenizer at a time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6kWs8ksxSpC"
      },
      "source": [
        "##### IndicBERT tokenizer with max word length 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbkybA0L3FB9",
        "outputId": "a3d33a1d-4eb0-48b3-bba7-2ca7a4e1e993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some of the Tokens: ['[CLS]', '▁सव', 'पन', '▁द', 'ख', 'वण', '▁आण', '▁आश', 'व', 'सन']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "token_ids = tokenizer.encode(corpus, max_length=1000, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "extracted_tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n",
        "print(\"Some of the Tokens:\", extracted_tokens[:10])\n",
        "model_name=\"indicBERT_word1000/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRt8LDzIxjr6"
      },
      "source": [
        "##### IndicBERT tokenizer with max word length 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svLPqBR6xkId",
        "outputId": "dc419f58-8dea-48f6-990f-9ca115a8c019"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Some of the Tokens: ['[CLS]', '▁सव', 'पन', '▁द', 'ख', 'वण', '▁आण', '▁आश', 'व', 'सन']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
        "token_ids = tokenizer.encode(corpus, max_length=2000, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
        "extracted_tokens = tokenizer.convert_ids_to_tokens(token_ids[0])\n",
        "print(\"Some of the Tokens:\", extracted_tokens[:10])\n",
        "model_name=\"indicBERT_word2000/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n45gcxf9x0Wa"
      },
      "source": [
        "#### Step4: Execute all of the following lines to get frequencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8EkXdkyWVJA",
        "outputId": "d21aac43-264b-49f7-8de7-eb7d7ac6fda9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Displaying some Tokens after unicode correction:  ['स्अव्अ', 'प्अन्अ', 'द्अ', 'ख्अ', 'व्अण्अ', 'आण्अ', 'आश्अ', 'व्अ', 'स्अन्अ', 'द्अण्अ']\n"
          ]
        }
      ],
      "source": [
        "#performing unicode correction on each token and storing it back\n",
        "for i in range(len(extracted_tokens)):\n",
        "    extracted_tokens[i]=unicode_correction(extracted_tokens[i])\n",
        "    extracted_tokens[i] = ''.join(extracted_tokens[i])\n",
        "\n",
        "#removing empty tokens if any\n",
        "extracted_tokens = [token for token in extracted_tokens if token != '']\n",
        "print(\"Displaying some Tokens after unicode correction: \",extracted_tokens[:10])\n",
        "\n",
        "#Creating the google drive directory to store frequencies\n",
        "indicBERT_Frequencies=\"/content/gdrive/MyDrive/\"+model_name\n",
        "os.makedirs(indicBERT_Frequencies, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvtKYrXEWWaG"
      },
      "source": [
        "##### Unigram frequencies of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wkfgZNBuWbBN"
      },
      "outputs": [],
      "source": [
        "unigram_counter = Counter(extracted_tokens)\n",
        "#sorting in descending order of count\n",
        "Desc_unigram_tokens = dict(sorted(unigram_counter.items(), key=lambda item: item[1], reverse=True))\n",
        "readable_unigram_tokens = json.dumps(Desc_unigram_tokens, ensure_ascii=False, indent=2)\n",
        "\n",
        "#Storing unigram frequencies of tokens in a JSON file in drive\n",
        "output_path = os.path.join(indicBERT_Frequencies, 'unigram_tokens.json')\n",
        "with open(output_path, 'w') as json_file:\n",
        "    json_file.write(readable_unigram_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFyWxOvDGiXm"
      },
      "source": [
        "##### bigram frequencies of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "UVyy6V8jGnQd"
      },
      "outputs": [],
      "source": [
        "bigram_tokens = zip(extracted_tokens, extracted_tokens[1:])\n",
        "bigram_counter = Counter(bigram_tokens)\n",
        "# Converting tuple keys to strings for JSON serialization\n",
        "bigram_counter = {str(key): value for key, value in bigram_counter.items()}\n",
        "\n",
        "\n",
        "# Storing bigram frequencies of tokens in a JSON file in drive\n",
        "readable_bigram_tokens = json.dumps(bigram_counter, ensure_ascii=False, indent=2)\n",
        "output_path = os.path.join(indicBERT_Frequencies, 'bigram_tokens.json')\n",
        "with open(output_path, 'w') as json_file:\n",
        "    json_file.write(readable_bigram_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDlVwe3TBnP4"
      },
      "source": [
        "##### bigram frequencies of characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GtaJuEOe5Rdl"
      },
      "outputs": [],
      "source": [
        "Bigram_Char_counts={}\n",
        "sequential_bigram_char_corpus=[]\n",
        "\n",
        "for token in extracted_tokens:\n",
        "    if not token:        #empty token\n",
        "        continue\n",
        "    token_char_count=character_count(token)\n",
        "    sentence_Bigramchar_count,bigram_chars_in_sentences=bigram_characters(token)\n",
        "\n",
        "    sequential_bigram_char_corpus.append(bigram_chars_in_sentences)        #append bigram character  of token to bigram characters of corpus list\n",
        "\n",
        "\n",
        "\n",
        "    for key in sentence_Bigramchar_count:\n",
        "        if(key in Bigram_Char_counts):\n",
        "            Bigram_Char_counts[key]+=sentence_Bigramchar_count[key]\n",
        "        else:\n",
        "            Bigram_Char_counts[key]=sentence_Bigramchar_count[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ZB32NXAEBahZ"
      },
      "outputs": [],
      "source": [
        "#Sorting in descending order of their frequencies\n",
        "Desc_bigramChar_count = dict(sorted(Bigram_Char_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "#storing bigram frequencies of characters in json file\n",
        "output_path_bigram_chars = os.path.join(indicBERT_Frequencies, 'bigram_chars.json')\n",
        "# Manually format the content for readability\n",
        "formatted_content_bigram_chars = json.dumps(Desc_bigramChar_count, indent=2, ensure_ascii=False)\n",
        "with open(output_path_bigram_chars, 'w', encoding='utf-8') as json_file:\n",
        "    json_file.write(formatted_content_bigram_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVzDPgM1CNIJ"
      },
      "source": [
        "##### Bigram frequencies of syllables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ldognAYrC-4F"
      },
      "outputs": [],
      "source": [
        "Bigram_syl_counts={}        #to measure overall bigram syllable count of corpus\n",
        "sequential_bigram_syl_corpus=[]     #to get all bigram syllables in same order of corpus\n",
        "\n",
        "for token in extracted_tokens:\n",
        "    if not token:\n",
        "        continue\n",
        "    bigram_syl_in_token=[]\n",
        "    token_syl_count=syllable_count(token)\n",
        "    token_bigramSyl_count,bigram_syl_in_token=bigram_syllable(token)\n",
        "\n",
        "    sequential_bigram_syl_corpus.append(bigram_syl_in_token)        #append bigram syllable of sentence to bigram syllables of corpus list\n",
        "\n",
        "    for key in token_bigramSyl_count:\n",
        "        if(key in Bigram_syl_counts):\n",
        "            Bigram_syl_counts[key]+=token_bigramSyl_count[key]\n",
        "        else:\n",
        "            Bigram_syl_counts[key]=token_bigramSyl_count[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uNdVKlquEGo2"
      },
      "outputs": [],
      "source": [
        "Desc_bigramSyl_count = dict(sorted(Bigram_syl_counts.items(), key=lambda item: item[1], reverse=True))\n",
        "output_path_bigram_syllables = os.path.join(indicBERT_Frequencies, 'bigram_syllables.json')\n",
        "formatted_content_bigram_syllables = json.dumps(Bigram_syl_counts, indent=2, ensure_ascii=False)\n",
        "with open(output_path_bigram_syllables, 'w', encoding='utf-8') as json_file:\n",
        "    json_file.write(formatted_content_bigram_syllables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "vnuLFibr2D-G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FdzASd3t2zgK",
        "3VJzu-3cxa6X"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
