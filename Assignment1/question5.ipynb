{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akash\\anaconda3\\envs\\iitkMtech\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: must execute this cell. \n",
    "As it contains definitions of functions which are used in further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "swar = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ',  'ए', 'ऐ', 'ओ', 'औ','ॲ', 'ऑ','अं', 'अः']\n",
    "\n",
    "halant_vyanjana_map = {'क्': 'क', 'ख्': 'ख', 'ग्': 'ग', 'घ्': 'घ',\n",
    "                        'ङ्': 'ङ', 'च्': 'च', 'छ्': 'छ', 'ज्': 'ज',\n",
    "                        'झ्': 'झ', 'ञ्': 'ञ', 'ट्': 'ट', 'ठ्': 'ठ',\n",
    "                        'ड्': 'ड', 'ढ्': 'ढ', 'ण्': 'ण', 'त्': 'त',\n",
    "                        'थ्': 'थ', 'द्': 'द', 'ध्': 'ध', 'न्': 'न',\n",
    "                        'प्': 'प', 'फ्': 'फ', 'ब्': 'ब', 'भ्': 'भ',\n",
    "                        'म्': 'म', 'य्': 'य', 'र्': 'र', 'ल्': 'ल',\n",
    "                        'व्': 'व', 'श्': 'श', 'ष्': 'ष', 'स्': 'स',\n",
    "                        'ह्': 'ह', 'ळ्': 'ळ', 'क्ष्': 'क्ष', 'ज्ञ्': 'ज्ञ'}\n",
    "\n",
    "\n",
    "swar_matra_map={'ा':'आ', 'ि':'इ', 'ी':'ई', 'ु':'उ', 'ू':'ऊ', 'े':'ए', 'ै':'ऐ', 'ो':'ओ', 'ौ': 'औ', 'ॅ':'ॲ', 'ॉ':'ऑ',  'ं':'अं', 'ः': 'अः'}\n",
    "reversed_swar_matra_map = {'आ': 'ा', 'इ': 'ि', 'ई': 'ी', 'उ': 'ु', 'ऊ': 'ू', 'ए': 'े', 'ऐ': 'ै', 'ओ': 'ो', 'औ': 'ौ', 'ॲ': 'ॅ', 'ऑ': 'ॉ', 'अं': 'ं', 'अः': 'ः'}\n",
    "\n",
    "\n",
    "specials= { 'ँ':['ॲ','अं']}\n",
    "\n",
    "\n",
    "vyanjana_map={'क': 'क्', 'ख': 'ख्', 'ग': 'ग्', 'घ': 'घ्', \n",
    "                        'ङ': 'ङ्', 'च': 'च्', 'छ': 'छ्', 'ज': 'ज्', \n",
    "                        'झ': 'झ्', 'ञ': 'ञ्', 'ट': 'ट्', 'ठ': 'ठ्', \n",
    "                        'ड': 'ड्', 'ढ': 'ढ्', 'ण': 'ण्', 'त': 'त्', \n",
    "                        'थ': 'थ्', 'द': 'द्', 'ध': 'ध्', 'न': 'न्', \n",
    "                        'प': 'प्', 'फ': 'फ्', 'ब': 'ब्', 'भ': 'भ्', \n",
    "                        'म': 'म्', 'य': 'य्', 'र': 'र्', 'ल': 'ल्', \n",
    "                        'व': 'व्', 'श': 'श्', 'ष': 'ष्', 'स': 'स्', \n",
    "                        'ह': 'ह्', 'ळ': 'ळ्', 'क्ष': 'क्ष्', 'ज्ञ': 'ज्ञ्'}\n",
    "\n",
    "#function of unicode correction for question 2. This is to handle \" \"spaces also\n",
    "def unicode_correction(user_input):     #input can be word or sentence\n",
    "    corrected_unicodes=[]\n",
    "    length=len(user_input)\n",
    "    for i in range (length):\n",
    "        if(user_input[i] in swar):\n",
    "            corrected_unicodes.append(user_input[i])\n",
    "        elif(user_input[i] in vyanjana_map):   #if it is already a halant character e.g. त् is represented as 'त', '्' in given input\n",
    "            if(i<length-1 and (user_input[i+1]=='्' or user_input[i+1] in swar_matra_map)):  #i.e. if vyanjan is either halant or it has matra then \n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]])                          #writing only halant vyanjan \n",
    "\n",
    "            elif(i<length-1 and user_input[i+1] in specials):       #to handle बँ i.e. 'ब', 'ँ' this kind of thing. In this case I just have to insert ब् and not 'ब्' 'अ'\n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]])\n",
    "            else:                                                   #if vyanjan is actually a full vyanjan then\n",
    "                corrected_unicodes.append(vyanjana_map[user_input[i]]) #writing halant vyanjan and\n",
    "                corrected_unicodes.append('अ')                      #also writing swar अ after that\n",
    "\n",
    "        elif(user_input[i] in swar_matra_map):  #if character is some kind of matra then replacing it with swar\n",
    "            corrected_unicodes.append(swar_matra_map[user_input[i]])\n",
    "\n",
    "        elif (user_input[i] in specials):   #to handle 'ँ', we need to explicitly insert all swar associated with it. which is stored in specials\n",
    "            for j in specials[user_input[i]]:\n",
    "                corrected_unicodes.append(j)\n",
    "        elif(user_input[i]==\" \"):\n",
    "            corrected_unicodes.append(\" \")\n",
    "    \n",
    "    return corrected_unicodes\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    true_positives=len(set(actual) & set(predicted))\n",
    "    false_positives=len(set(predicted) - set(actual))\n",
    "    false_negatives=len(set(actual) - set(predicted))\n",
    "    \n",
    "    precision=true_positives/(true_positives+false_positives) if (true_positives+false_positives)>0 else 0\n",
    "    recall=true_positives/(true_positives+false_negatives) if (true_positives+false_negatives) > 0 else 0\n",
    "    f1_score=2 *(precision*recall)/(precision+recall) if (precision+recall) > 0 else 0\n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Reading and tokanizing (comma seperated) Ground truth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = \"corpus/question5_GroundTruth.txt\"\n",
    "# Open the file and read its content\n",
    "with open(ground_truth_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    content = file.read()\n",
    "ground_truth = content.split(',')\n",
    "#Removing whitespaces from each element in the list\n",
    "ground_truth = [text.strip() for text in ground_truth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth Tokens after unicode correction:  ['र्अव्इव्आर्ई', 'एन्अआर्अस्ईच्आ', 'ज्ओ प्अह्इल्आ', 'म्अस्उद्आ', 'ज्आह्ईर्अ झ्आल्आ', 'त्य्आन्उस्आर्अ', 'र्आज्य्आत्ईल्अ', 'क्ओट्ई ल्आख्अ', 'न्आग्अर्इक्आअंप्ऐक्ई', 'क्ओट्ई', 'ल्आख्अ', 'ल्ओक्आअंन्आ', 'अध्इक्अत्अ', 'न्आग्अर्इक्अत्व्आच्आ द्अर्ज्आ', 'ब्अह्आल्अ क्अर्अण्य्आत्अ', 'आल्आ आह्ए', 'ज्अर्ई श्अन्इव्आर्अ', 'व्अ', 'र्अव्इव्आर्अ', 'आप्अण्अ', 'च्अव्अर्अ', 'ज्ओर्अ व्अग्ऐर्ए', 'द्एत्अ', 'त्य्आअंच्य्आ', 'प्अर्इव्अर्त्अन्आच्य्आ', 'ब्ऐठ्अक्आ म्आर्अण्ए', 'स्उर्ऊ आह्ए', 'ब्ऐठ्अक्अ', 'ब्ओल्आव्अल्ई अस्अल्य्आच्ई', 'म्आह्इत्ई', 'आम्अद्आर्अ', 'अन्इल्अ ब्आब्अर्अ य्आअंन्ई', 'ढ्अव्अळ्ईत्अ', 'प्उर्अग्र्अस्त्आअंच्य्आ', 'म्अद्अत्अ', 'व्आट्अप्आच्ई', 'च्औक्अश्ई', 'स्उर्उ', 'ब्आज्आर्अ स्अम्इत्ईच्य्आ', 'आव्आर्आत्ईल्अ', 'ग्ओद्आम्आत्अ', 'व्अ', 'न्एम्आड्ए फ्ल्ऑट्अ', 'व्अस्त्ईत्ईल्अ', 'ग्ओद्आम्आत्अ', 'ब्आर्अद्आन्आच्ए', 'ग्अठ्ए', 'आढ्अळ्उन्अ आल्ए', 'न्इर्व्य्अस्अन्ई', 'आह्एत्अ', 'अम्एर्इक्एच्ए र्आष्ट्र्आध्य्अक्ष्अ', 'प्अह्आ', 'क्आय्अ', 'ब्अर्अळ्अल्आ', 'प्आक्अड्य्आअंच्आ प्अंत्अप्र्अध्आन्अ', 'इम्अर्आन्अ ख्आन्अ', 'म्उअंब्अई', 'द्इव्अस्अभ्अर्आत्ईल्अ', ' ज्अब्अर्अद्अस्त्अ ब्आत्अम्य्आ', ' फ्एब्र्उव्आर्ई ', 'आज्अ', 'द्इव्अस्अभ्अर्आम्अध्य्ए', 'अन्एक्अ', 'म्ओठ्य्आ घ्अड्आम्ओड्ई', 'घ्अड्अल्य्आ', 'एस्अट्ई', 'म्अह्आम्अंड्अळ्आल्आ', 'न्उक्अस्आन्अ भ्अर्अप्आईप्ओट्ई', ' ल्आख्अ  ह्अज्आर्अ', 'र्उप्अय्ए', ' ट्अक्क्ए', 'व्य्आज्आन्ए', 'द्एण्य्आच्ए', 'आद्एश्अ', 'द्एण्य्आत्अ आल्ए आह्एत्अब्आअंब्ऊ', 'ह्अस्त्अक्अल्आ क्एअंद्र्अ', '  ल्आख्अत्य्आम्उळ्ए', 'प्अश्च्इम्अ र्एल्व्एव्अर्ईल्अ', 'ल्ओक्अस्एव्आ', ' त्ए ', 'म्इन्इट्अं', 'उश्इर्आन्ए', 'ध्आव्अत्ईल्अय्आव्य्अत्इर्इक्त्अ', 'फ्अळ्अ', 'अश्आ', 'प्ओट्ॲश्इय्अम्अ', 'म्ॲग्न्एश्इय्अम्अ', 'ज्अस्त्अ', 'ल्ओह्अ', 'त्आअंब्ए', 'म्ॲअंग्अन्ईज्अ', 'म्ॲग्न्एश्इय्अम्अ', 'आण्इ', 'म्औल्अ म्ह्अण्ऊन्अ', 'घ्अट्अक्अ', 'स्अम्आव्इष्ट्अ आह्एएक्अद्आ', 'आल्ऊ प्अर्आठ्य्आच्य्आ', 'न्आव्आख्आल्ई', 'क्आह्ईस्अं', 'ख्आत्अ अस्अत्आन्आ', 'ल्ओण्अच्य्आच्ई', 'आठ्अव्अण्अ झ्आल्ईप्अंच्अव्ईस्अ', 'व्अर्ष्ए', 'स्ट्ॲम्अफ्अर्ड्अ ब्र्इज्अ', 'क्आय्अम्अ र्आह्इल्एत्उष्आर्अ श्एव्आळ्ए', 'क्आॲअंग्र्एस्अ', 'भ्अक्त्इस्उख्एअं', 'द्ओअंद्अ', 'व्आढ्एल्अ', 'श्उक्र्अव्आर्ई', 'स्आय्अंक्आळ्ई', 'च्अव्ह्आट्अ ग्अल्ल्ल्ई', 'य्एथ्ए', 'ह्इ घ्अट्अन्आ', 'घ्अड्अल्ई अस्ऊन्अ', 'ज्अख्अम्ई', 'य्उव्अक्आल्आ', 'उप्अच्आर्आस्आठ्ई', 'स्इव्ह्इल्अ ह्ऑस्प्इट्अल्अम्अध्य्ए', 'द्आख्अल्अ', 'क्अर्अण्य्आत्अ आल्एय्आआध्ई', 'अश्आच्अ प्र्अक्आर्ए', 'म्अह्आब्अल्अ म्इश्र्आ य्आअंच्य्आ', 'प्उत्र्आन्ए', 'क्आॲअंग्र्एस्अच्आ', 'ह्आत्अ', 'स्ओड्अल्आ ह्ओत्आह्अल्ल्ईच्य्आ', 'भ्आष्एत्अ', 'त्ई', 'ड्आऊन्अ म्आर्क्एट्अ', 'झ्आल्ईम्उल्आअंस्ओब्अत्अच्अ', 'प्आल्अक्आअंन्आ', 'स्आअंभ्आळ्अण्य्आच्ई', 'ज्अब्आब्अद्आर्ई ह्ई', 'व्इव्आह्ईत्अ', 'म्अह्इल्आअंच्ईह्ई', 'आह्ए', 'क्एव्अळ्अ', 'व्इव्आह्अ', 'झ्आल्आय्अ म्ह्अण्ऊन्अ', 'व्अद्ध्अ', 'प्आल्अक्आअंच्ई', 'ज्अब्आब्अद्आर्ई', 'त्य्आअंन्आ', 'झ्अट्अक्अत्आ', 'य्एण्आर्अ न्आह्ई अस्अं', 'ह्आय्अक्ओर्ट्आन्अं', 'म्ह्अट्अल्अंय्अअन्एक्आअंन्ई', 'आप्अल्ई', 'अघ्ओष्इत्अ स्अंप्अत्त्ई', 'त्य्आअंच्ए क्अर्म्अच्आर्ई', 'क्इअंव्आ', 'एज्अंट्अच्य्आ', 'म्आध्य्अम्आत्ऊन्अ', 'ज्अन्अध्अन्अ ख्आत्य्आअंम्अध्य्ए', 'ज्अम्आ क्एल्य्आच्आ', 'स्अर्अक्आर्अल्आ', 'स्अंश्अय्अ आह्एम्आत्र्अ', 'म्अट्अक्य्आत्अ', 'म्अंद्ई', 'आल्य्आम्उळ्ए', 'ज्इल्ह्आभ्अर्अ', 'ऑन्अल्आईन्अ ज्उग्आर्अ', 'च्आअंग्अल्एच्अ', 'त्एज्ईत्अ', 'आल्य्आच्ए', 'द्इस्अत्अ आह्एत्अश्र्ई म्अह्आल्अक्ष्म्ई ब्ॲअंक्एत्अ', 'प्र्अज्आस्अत्त्आक्अ द्इन्अ', 'उत्स्आह्आत्अत्य्आम्उळ्अ', 'प्ऊर्व्ईच्ई', 'अन्एक्अस्त्अर्ईय्अ', 'ज्अट्ईल्अ', 'क्अर्अप्र्अण्आल्ई', 'आत्आ', 'प्आच्अ', 'स्त्अर्आअंम्अध्य्ए', 'व्इभ्आग्अल्ई', 'ग्एल्ई आह्एप्उढ्ईल्अ व्अर्ग्अ', 'य्आ भ्आग्आत्अ', 'प्र्अस्अंग्ओप्आत्अ', ' व्एळ्आ', 'आठ्अव्अड्य्आत्ऊन्अ', 'च्आल्अत्ए प्आह्इज्ए', 'ज्ए  ह्ए उत्त्अम्अ', 'श्आर्ईर्इक्अ', 'आक्आर्अ', 'श्अर्ईर्आत्अ', 'ठ्एव्अण्ए', 'आव्अश्य्अक्अ', 'क्इम्आन्अ आह्ए', 'आप्अण्अ', 'स्अम्अस्य्आ भ्आग्आत्अ', 'एक्अ', 'स्न्आय्ऊ ग्अट्अ', 'क्आर्य्अ', 'क्अर्ऊ श्अक्अत्ओ', 'आण्इ', ' व्य्आय्आम्अ', 'स्उर्ऊ', 'क्अर्ऊ श्अक्अत्आ', 'त्अप्अश्ईल्अ', 'स्अर्व्अ', 'स्न्आय्ऊ ग्अट्अ']\n",
      "Total  203  ground truth tokens\n"
     ]
    }
   ],
   "source": [
    "#performing unicode correction ground truth token and storing it back\n",
    "for i in range(len(ground_truth)):\n",
    "    ground_truth[i]=unicode_correction(ground_truth[i])\n",
    "    ground_truth[i] = ''.join(ground_truth[i])\n",
    "#removing empty tokens if any\n",
    "ground_truth = [token for token in ground_truth if token != '']\n",
    "print(\"Ground truth Tokens after unicode correction: \",ground_truth)\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Reading question5 corpus (sentences from question 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"corpus/question5_corpus.txt\"\n",
    "# Open the file and read its content\n",
    "with open(corpus_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    corpus = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Finding Performances of each of tokenizer's tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokanizing with mBERT with max_length=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens after unicode correction:  ['र्अ', 'व्इ', 'व्आर्ई', 'एन्अ', 'आ', 'र्अ', 'स्ई', 'च्आ', 'ज्ओ', 'प्अ']\n",
      "Total  203  ground truth tokens\n",
      "Total  850  mBERT1000 calculated tokens\n",
      "Precision:  0.0798611111111111\n",
      "Recall:  0.116751269035533\n",
      "F1_score 0.09484536082474226\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#Tokenizing the corpus with max length = 1000\n",
    "tokens_ids = tokenizer.encode(corpus, max_length=1000, truncation=True)\n",
    "#getting tokens from ids\n",
    "mBERT1000_tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(mBERT1000_tokens)):\n",
    "    mBERT1000_tokens[i]=unicode_correction(mBERT1000_tokens[i])\n",
    "    mBERT1000_tokens[i] = ''.join(mBERT1000_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "mBERT1000_tokens = [token for token in mBERT1000_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",mBERT1000_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(mBERT1000_tokens),\" mBERT1000 calculated tokens\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,mBERT1000_tokens)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokanizing with mBERT with max_length=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens after unicode correction:  ['र्अ', 'व्इ', 'व्आर्ई', 'एन्अ', 'आ', 'र्अ', 'स्ई', 'च्आ', 'ज्ओ', 'प्अ']\n",
      "Total  203  ground truth tokens\n",
      "Total  850  mBERT2000 calculated tokens\n",
      "\n",
      "Precision:  0.0798611111111111\n",
      "Recall:  0.116751269035533\n",
      "F1_score 0.09484536082474226\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#Tokenizing the corpus with max length = 1000\n",
    "tokens_ids = tokenizer.encode(corpus, max_length=1000, truncation=True)\n",
    "#getting tokens from ids\n",
    "mBERT2000_tokens = tokenizer.convert_ids_to_tokens(tokens_ids)\n",
    "\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(mBERT2000_tokens)):\n",
    "    mBERT2000_tokens[i]=unicode_correction(mBERT2000_tokens[i])\n",
    "    mBERT2000_tokens[i] = ''.join(mBERT2000_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "mBERT2000_tokens = [token for token in mBERT2000_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",mBERT2000_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(mBERT2000_tokens),\" mBERT2000 calculated tokens\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,mBERT2000_tokens)\n",
    "print(\"\\nPrecision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokanization using BPE with v=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens after unicode correction:  ['र्अव्इव्आर्ई', 'एन्अआर्अस्ईच्आ', 'ज्ओ', 'प्अह्इल्आ', 'म्अस्उद्आ', 'ज्आह्ईर्अ', 'झ्आल्आ', 'त्य्आन्उस्आर्अ', 'र्आज्य्आत्ईल्अ', 'क्ओट्ई']\n",
      "Total  203  ground truth tokens\n",
      "Total  296  BPE1000 calculated tokens\n",
      "\n",
      "Precision:  0.45318352059925093\n",
      "Recall:  0.6142131979695431\n",
      "F1_score 0.5215517241379309\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training BPE model with vocabulary size V = 1000\n",
    "\n",
    "#This took around 4 minues to execute this cell\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/question5_corpus.txt --model_prefix=bpe_model_1000 --model_type=bpe --vocab_size=1000')\n",
    "#Loading BPE model (V = 1000)\n",
    "sp_bpe_1000 = spm.SentencePieceProcessor()\n",
    "sp_bpe_1000.load('bpe_model_1000.model')\n",
    "#Tokenizing with BPE model (V = 1000)\n",
    "BPE1000_tokens = sp_bpe_1000.encode_as_pieces(corpus)\n",
    "\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(BPE1000_tokens)):\n",
    "    BPE1000_tokens[i]=unicode_correction(BPE1000_tokens[i])\n",
    "    BPE1000_tokens[i] = ''.join(BPE1000_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "BPE1000_tokens = [token for token in BPE1000_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",BPE1000_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(BPE1000_tokens),\" BPE1000 calculated tokens\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,BPE1000_tokens)\n",
    "print(\"\\nPrecision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokanization BPE with v=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying some Tokens after unicode correction:  ['र्अव्इव्आर्ई', 'एन्अआर्अस्ईच्आ', 'ज्ओ', 'प्अह्इल्आ', 'म्अस्उद्आ', 'ज्आह्ईर्अ', 'झ्आल्आ', 'त्य्आन्उस्आर्अ', 'र्आज्य्आत्ईल्अ', 'क्ओट्ई']\n",
      "Total  203  ground truth tokens\n",
      "Total  290  BPE2000 calculated tokens\n",
      "\n",
      "Precision:  0.47509578544061304\n",
      "Recall:  0.6294416243654822\n",
      "F1_score 0.5414847161572052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training BPE model with vocabulary size V = 2000\n",
    "\n",
    "#This took around 4 minues to execute this cell\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/question5_corpus.txt --model_prefix=bpe_model_2000 --model_type=bpe --vocab_size=2000')\n",
    "#Loading BPE model (V = 2000)\n",
    "sp_bpe_2000 = spm.SentencePieceProcessor()\n",
    "sp_bpe_2000.load('bpe_model_2000.model')\n",
    "#Tokenizing with BPE model (V = 2000)\n",
    "BPE2000_tokens = sp_bpe_2000.encode_as_pieces(corpus)\n",
    "\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(BPE2000_tokens)):\n",
    "    BPE2000_tokens[i]=unicode_correction(BPE2000_tokens[i])\n",
    "    BPE2000_tokens[i] = ''.join(BPE2000_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "BPE2000_tokens = [token for token in BPE2000_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",BPE2000_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(BPE2000_tokens),\" BPE2000 calculated tokens\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,BPE2000_tokens)\n",
    "print(\"\\nPrecision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokanization by unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the extracted Tokens: ['▁1', '.', '▁रविवार', 'ी', '▁ए', 'न', 'आ', 'रस', 'ी', 'चा']\n",
      "Displaying some Tokens after unicode correction:  ['र्अव्इव्आर्अ', 'ई', 'ए', 'न्अ', 'आ', 'र्अस्अ', 'ई', 'च्आ', 'ज्ओ', 'प्अ']\n",
      "Total  203  ground truth tokens\n",
      "Total  852   tokens calculated using Unigram Tokenizer\n",
      "\n",
      "Precision:  0.06060606060606061\n",
      "Recall:  0.08121827411167512\n",
      "F1_score 0.06941431670281994\n"
     ]
    }
   ],
   "source": [
    "#training Unigram model with vocab size V = 1000\n",
    "\n",
    "#if want to use another corpus. then provide path in input=\"\" in below line\n",
    "spm.SentencePieceTrainer.Train('--input=corpus/question5_corpus.txt --model_prefix=unigram_model --model_type=unigram --vocab_size=381')\n",
    "\n",
    "sp_unigram = spm.SentencePieceProcessor()\n",
    "sp_unigram.load('unigram_model.model')\n",
    "\n",
    "#Tokenization with Unigram model\n",
    "unigram_tokens = sp_unigram.encode_as_pieces(corpus)\n",
    "\n",
    "print(\"Some of the extracted Tokens:\", unigram_tokens[:10])\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(unigram_tokens)):\n",
    "    unigram_tokens[i]=unicode_correction(unigram_tokens[i])\n",
    "    unigram_tokens[i] = ''.join(unigram_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "unigram_tokens = [token for token in unigram_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",unigram_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(unigram_tokens),\"  tokens calculated using Unigram Tokenizer\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,unigram_tokens)\n",
    "print(\"\\nPrecision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization using Whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some of the extracted Tokens: ['1.', 'रविवारी', 'एनआरसीचा', 'जो', 'पहिला', 'मसुदा', 'जाहीर', 'झाला', 'त्यानुसार', 'राज्यातील']\n",
      "Displaying some Tokens after unicode correction:  ['र्अव्इव्आर्ई', 'एन्अआर्अस्ईच्आ', 'ज्ओ', 'प्अह्इल्आ', 'म्अस्उद्आ', 'ज्आह्ईर्अ', 'झ्आल्आ', 'त्य्आन्उस्आर्अ', 'र्आज्य्आत्ईल्अ', 'क्ओट्ई']\n",
      "Total  203  ground truth tokens\n",
      "Total  288   tokens calculated using whiteSpace Tokenizer\n",
      "\n",
      "Precision:  0.4864864864864865\n",
      "Recall:  0.6395939086294417\n",
      "F1_score 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "whiteSpaced_tokens = corpus.split()\n",
    "print(\"Some of the extracted Tokens:\", whiteSpaced_tokens[:10])\n",
    "#performing unicode correction on each token and storing it back\n",
    "for i in range(len(whiteSpaced_tokens)):\n",
    "    whiteSpaced_tokens[i]=unicode_correction(whiteSpaced_tokens[i])\n",
    "    whiteSpaced_tokens[i] = ''.join(whiteSpaced_tokens[i])\n",
    "\n",
    "#removing empty tokens if any\n",
    "whiteSpaced_tokens = [token for token in whiteSpaced_tokens if token != '']\n",
    "print(\"Displaying some Tokens after unicode correction: \",whiteSpaced_tokens[:10])\n",
    "print(\"Total \",len(ground_truth),\" ground truth tokens\")\n",
    "print(\"Total \",len(whiteSpaced_tokens),\"  tokens calculated using whiteSpace Tokenizer\")\n",
    "\n",
    "precision, recall, f1_score=confusion_matrix(ground_truth,whiteSpaced_tokens)\n",
    "print(\"\\nPrecision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"F1_score\",f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iitkMtech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
